<!DOCTYPE html>
<html><head>

  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  </style>

  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <script type="text/javascript" async="" src="https://www.google-analytics.com/analytics.js"></script><script type="text/javascript" src="js/hidebib.js"></script>
  <title>Pou-Chun Kung</title>
  <meta name="Pou-Chun Kung's Homepage" http-equiv="Content-Type" content="Pou-Chun Kung's Homepage">
  <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" rel="stylesheet" type="text/css">
  <!-- Start : Google Analytics Code -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-PHEY63QC7J"></script>
	<script>
  	window.dataLayer = window.dataLayer || [];
  	function gtag(){dataLayer.push(arguments);}
  	gtag('js', new Date());

  	gtag('config', 'G-PHEY63QC7J');
	</script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>


</head>


<!-- =================== Bio =================== -->
<body data-new-gr-c-s-check-loaded="14.1034.0" data-gr-ext-installed="">
<table width="1240" border="0" align="center" cellspacing="0" cellpadding="20">
<tbody><tr><td>

<p align="center">
    <pageheading> Pou-Chun (Frank) Kung</pageheading><br>
    <font id="email" style="display:inline;">pckung@umich.edu</font>
  </p><table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  

  <tbody><tr>
    <td width="32%" valign="top"><a href="images/frank.jpg"><img src="images/frank.jpg" width="100%" style="border-radius:15px"></a>
    <p align="center">
    | 
    <a href="files/CV_Pou_Chun_Kung_12_12.pdf">CV</a> |
    <!-- <a href="https://scholar.google.com/citations?hl=en&amp;user=xE3WSuYAAAAJ">Google Scholar</a> |<br>| -->
    <a href="https://github.com/kungfrank">Github</a> |
    <!-- <a href="https://twitter.com/johnsonwang0810">Twitter</a> | -->
    </p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p></p>
    <div style="font-size:12.5pt;" align="justify">
      <p>
        I am a PhD student in University of Michigan--Ann Arbor (UM). I am currently a research assistant at <a href="https://fcav.engin.umich.edu/">Ford Center for Autonomous Vehicle (FCAV)</a> advised by <a href="https://sites.google.com/umich.edu/kskin">Prof. Katie Skinner</a>. 
        Before that, I worked in Self-Driving Car Team at Industrial Technology Research Institute (ITRI), Taiwan, with <a href="https://sites.google.com/site/chiehchihbobwang/home?authuser=0"> Prof. Chieh-Chih (Bob) Wang</a>. 
        I received my M.Sc. in Robotics at National Yang Ming Chiao Tung University (NYCU),
        advised by Prof. Chieh-Chih (Bob) Wang
        and <a href="http://gpl.cs.nctu.edu.tw/Steve-Lin/">Prof. Wen-Chieh (Steve) Lin</a>.
        Before I started my master's study, I had a wonderful time working with <a href="https://natanaso.github.io/">Prof. Nikolay Atanasov</a> at UC San Diego.
        Prior to that, I received my B.Sc. in Electrical Engineering from National Sun Yat-sen University (NSYSU).
        During my undergraduate study, I was an undergraduate research assistant advised by Prof. Kao-Shing Hwang.
      </p>
      <p>
        My research interest lies in the intersection of robotics and computer vision. My research is focused on robot perception and state estimation.
        Particularly, I am fascinated with building robust robotic systems and joint research on machine learning and dynamic simultaneous localization and mapping (SLAM).
      </p>
      
    </div>
    </td>
  </tr>
</tbody></table>


<!-- =================== Experience =================== -->
<table width="100%" align="center" style="margin-left:10px" cellspacing="0" cellpadding="0" border="0">
  <tbody><tr>
    <th width="20%" valign="center" align="center">
      <img src="images/UM.png" alt="sym" width="40%">
      <p style="line-height:1.3; font-size:12pt">U-M<br>PhD in Robotics<br>Sept. 22 - Now</p>
    </th>
    <th width="20%" valign="center" align="center">
      <img src="images/nctu.png" alt="sym" width="40%">
      <p style="line-height:1.3; font-size:12pt">NYCU (NCTU)<br>M.Sc. in Robotics<br>Sept. 19 - Sept. 21</p>
    </th>
    <th width="20%" valign="center" align="center">
      <img src="images/ucsd-logo.jpg" alt="sym" width="70%">
      <p style="line-height:1.3; font-size:12pt">UCSD<br>Research Intern<br>Jul. 19 - Sept. 19</p>
    </th>
    <th width="20%" valign="center" align="center">
      <img src="images/nsysu.png" alt="sym" width="40%">
      <p style="line-height:1.3; font-size:12pt">NSYSU<br>B.Sc. in EE<br>Sept. 15 - July. 19</p>
    </th>
  </tr>
</tbody></table>


<!-- =================== News =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td>
    <sectionheading>&nbsp;&nbsp;News</sectionheading>
    <ul>
    <li>[05/2024] Join Ford/Latitude AI as an intern in perception team<br></li>
    <li>[05/2024] LONER won RA-L 2023 <span style="color: red;">Best Paper Award!</span><br></li>
    <li>[10/2024] One paper accepted by CVPR 2024 workshop<br></li>
    <li>[10/2023] One paper accepted by RA-L 2023<br></li>
    <li>[09/2022] Join Ford Center for Autonomous Vehicles (FCAV) lab at UM <br></li>
    <li>[03/2022] Join Taiwan (R.O.C) Marine Corps. Semper Fi! <br></li>
    <li>[01/2022] Join Self-Driving Car Team at Industrial Technology Research Institute (ITRI)<br></li>
    <li>[01/2022] My paper on radar occupancy prediction is accepted by RA-L'22.<br></li>
    <li>[09/2021] Receive The Phi Tau Phi Scholastic Honor.<br></li>
    <!-- <li>[09/2021] Receive my M.Sc. degree at NYCU. Thanks to Bob Wang, Steve Lin, and all my colleagues in PAL.<br></li> -->
    <!--<li><b><FONT color=red>[01/2022] My paper on radar occupancy prediction is accepted by RA-L'22.</FONT></b> <br></li>
    <li>[02/2021] One paper on radar odometry/scan matching accepted by ICRA'21.<br></li>
    <li>[09/2019] Start my graduate study at NYCU (NCTU) advised by <a href="https://sites.google.com/site/chiehchihbobwang/home?authuser=0">Prof. Chieh-Chih (Bob) Wang</a>.<br></li>
    <li>[09/2019] My wonderful journey at UCSD ends! Thanks Prof. Nikolay, Arash, and all members in ERL.<br></li>
    <li>[07/2019] Start my research internship at UCSD working with <a href="https://natanaso.github.io/">Prof. Nikolay Atanasov</a>.<br></li>
    -->

    <!--li>[07/2019] One paper on video generation accepted by ICCV'19!<br></li-->
    <!--li>[06/2019] One paper on stereo and LiDAR fusion accepted by IROS'19!<br></li-->
    <!--li>[06/2019] Start my research internship at Uber ATG working with <a href="http://www.cs.toronto.edu/~urtasun/">Prof. Raquel Urtasun</a>.<br></li-->
    <!--li>[01/2019] One paper on depth completion accepted by ICRA'19!<br></li-->
    <!--li>[06/2018] One paper accepted by ECCV'18.<br></li-->
    <!--li>[01/2018] One paper on robot place recognition accepted by ICRA'18.<br></li-->
    <!--li>[09/2017] Start my graduate study at NTHU advised by <a href="https://aliensunmin.github.io/">Prof. Min Sun</a>.<br></li-->
    </ul>
  </td></tr>
</tbody></table>


<!-- =================== Publication =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tbody>
  <!-- #################################### -->
  <tr>
    <td width="33%" valign="top" align="center">
      <a href="https://openaccess.thecvf.com/content/CVPR2024W/NRI/papers/Kung_SAD-GS_Shape-aligned_Depth-supervised_Gaussian_Splatting_CVPRW_2024_paper.pdf"></a>
      <img src="images/sad-gs.png" alt="sym" width="290" style="border-radius:10px">
    </td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2112.04282" id="SADGS">
      <heading>SAD-GS: Shape-aligned Depth-supervised Gaussian Splatting</heading></a><br>
      <u><b>Pou-Chun Kung</b></u>, Seth Isaacson, Ram Vasudevan, and Katherine A. Skinner<br>
      CVPR 2024 Workshop, Seattle
      </p>

      <div class="paper" id="sadgs">
        <a href="javascript:toggleblock('sadgs_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('sadgs')" class="togglebib">bibtex</a> |
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/NRI/papers/Kung_SAD-GS_Shape-aligned_Depth-supervised_Gaussian_Splatting_CVPRW_2024_paper.pdf">paper</a> |
        <a href="https://umautobots.github.io/sad_gs">project page</a>
        <p align="justify"> <i id="sadgs_abs" style="display: none;">This paper proposes SAD-GS, a depth-supervised Gaussian Splatting (GS) method that provides accurate 3D geometry reconstruction by introducing a shape-aligned depth supervision strategy. Depth information is widely used in various GS applications, such as dynamic scene reconstruction, real-time simultaneous localization and mapping, and few-shot reconstruction. However, existing depth-supervised methods for GS all focus on the center and neglect the shape of Gaussians during training. This oversight can result in inaccurate surface geometry in the reconstruction and can harm downstream tasks like novel view synthesis, mesh reconstruction, and robot path planning. 
          To address this, this paper proposes a shape-aligned loss, which aims to produce a smooth and precise reconstruction by adding extra constraints to the Gaussian shape.
          The proposed method is evaluated qualitatively and quantitatively on two publicly available datasets. 
          The evaluation demonstrates that the proposed method provides state-of-the-art novel view rendering quality and mesh accuracy compared to existing depth-supervised GS methods..</i></p>
        <pre xml:space="preserve" style="display: none;">@inproceedings{kung2024sad,
          title={SAD-GS: Shape-aligned Depth-supervised Gaussian Splatting},
          author={Kung, Pou-Chun and Isaacson, Seth and Vasudevan, Ram and Skinner, Katherine A},
          booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
          pages={2842--2851},
          year={2024}
        }
        </pre>
      </div>
    </td>
  </tr>
  <!-- #################################### -->
  <tr>
    <td width="33%" valign="top" align="center">
      <a href="https://arxiv.org/pdf/2309.04937"></a>
      <img src="images/loner.gif" alt="sym" width="295" height="170" style="border-radius:10px">
    </td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/pdf/2309.04937" id="R2L">
      <heading>LONER: LiDAR Only Neural Representations for Real-Time SLAM</heading></a><br>
      Seth Isaacson*, <u><b>Pou-Chun Kung*</b></u>, Ram Vasudevan, and Katherine A. Skinner<br>
      RA-L 2023, <strong><span style="color: red;">Best Paper Award!</span></strong> <br>
      Presented in ICRA 2024 Yokohama
      </p>

      <div class="paper" id="r2l">
        <a href="javascript:toggleblock('r2l_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('r2l')" class="togglebib">bibtex</a> |
        <a href="https://arxiv.org/pdf/2309.04937">arXiv</a> |
        <a href="https://umautobots.github.io/loner">project page</a>
        <p align="justify"> <i id="r2l_abs" style="display: none;">This paper proposes LONER, the first real-time
          LiDAR SLAM algorithm that uses a neural implicit scene
          representation. Existing implicit mapping methods for LiDAR
          show promising results in large-scale reconstruction, but either
          require groundtruth poses or run slower than real-time. In
          contrast, LONER uses LiDAR data to train an MLP to estimate
          a dense map in real-time, while simultaneously estimating the
          trajectory of the sensor. To achieve real-time performance, this
          paper proposes a novel information-theoretic loss function that
          accounts for the fact that different regions of the map may
          be learned to varying degrees throughout online training. The
          proposed method is evaluated qualitatively and quantitatively on
          two open-source datasets. This evaluation illustrates that the proposed loss function converges faster and leads to more accurate
          geometry reconstruction than other loss functions used in depthsupervised neural implicit frameworks. Finally, this paper shows
          that LONER estimates trajectories competitively with state-ofthe-art LiDAR SLAM methods, while also producing dense maps
          competitive with existing real-time implicit mapping methods that
          use groundtruth poses.</i></p>
        <pre xml:space="preserve" style="display: none;">@article{isaacson2023loner,
          title={Loner: Lidar only neural representations for real-time slam},
          author={Isaacson, Seth and Kung, Pou-Chun and Ramanagopal, Mani and Vasudevan, Ram and Skinner, Katherine A},
          journal={IEEE Robotics and Automation Letters},
          year={2023},
          publisher={IEEE}
        }
        </pre>
      </div>
    </td>
  </tr>
    <!-- #################################### -->
  <tr>
    <td width="33%" valign="top" align="center">
      <a href="https://arxiv.org/abs/2112.04282"></a>
      <img src="images/r2l.gif" alt="sym" width="295" height="170" style="border-radius:10px">
    </td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2112.04282" id="R2L">
      <heading>Radar Occupancy Prediction with Lidar Supervision while Preserving Long-Range Sensing and Penetrating Capabilities</heading></a><br>
      <u><b>Pou-Chun Kung</b></u>, Chieh-Chih Wang, Wen-Chieh Lin<br>
      <!--li>Under revision in RA-L 2022</li-->
      RA-L 2022, Philadelphia
      </p>

      <div class="paper" id="r2l">
        <a href="javascript:toggleblock('r2l_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('r2l')" class="togglebib">bibtex</a> |
        <a href="https://arxiv.org/abs/2112.04282">arXiv</a> |
        <a href="https://youtu.be/PpMg6t60yik">video</a>
        <p align="justify"> <i id="r2l_abs" style="display: none;">Radar shows great potential for autonomous driving by accomplishing long-range sensing under diverse weather conditions. But radar is also a particularly challenging sensing modality due to the radar noises. Recent works have made enormous progress in classifying free and occupied spaces in radar images by leveraging lidar label supervision. However, there are still several unsolved issues. Firstly, the sensing distance of the results is limited by the sensing range of lidar. Secondly, the performance of the results is degenerated by lidar due to the physical sensing discrepancies between the two sensors. For example, some objects visible to lidar are invisible to radar, and some objects occluded in lidar scans are visible in radar images because of the radar's penetrating capability. These sensing differences cause false positive and penetrating capability degeneration, respectively. <br>

        &nbsp;&nbsp;In this paper, we propose training data preprocessing and polar sliding window inference to solve the issues. The data preprocessing aims to reduce the effect caused by radar-invisible measurements in lidar scans. The polar sliding window inference aims to solve the limited sensing range issue by applying a near-range trained network to the long-range region. Instead of using common Cartesian representation, we propose to use polar representation to reduce the shape dissimilarity between long-range and near-range data. We find that extending a near-range trained network to long-range region inference in the polar space has 4.2 times better IoU than in Cartesian space. Besides, the polar sliding window inference can preserve the radar penetrating capability by changing the viewpoint of the inference region, which makes some occluded measurements seem non-occluded for a pretrained network.</i></p>
        <pre xml:space="preserve" style="display: none;">@inproceedings{kung2021radar,
          Author = {Pou-Chun Kung and
                    Chieh-Chih Wang and
                    Wen-Chieh Lin},
          Title = {Radar Occupancy Prediction with Lidar Supervision while Preserving Long-Range Sensing and Penetrating Capabilities},
          Booktitle = {arXiv:2112.04282}
        }
        </pre>
      </div>
    </td>
  </tr>
  <!-- #################################### -->
  <!-- <tr>
    <td width="33%" valign="top" align="center">
      <a href="#"></a>
      <img src="images/r2l_3d_comp.gif" alt="sym" width="300" height="240" style="border-radius:15px">
    </td>
    <td width="67%" valign="top">
      <p><a id="R2L3D">
      <heading>Target 3D Shape Estimation from a Low-Cost 4D Radar Module</heading></a><br>
      Chien-Cheng Fang, <u><b>Pou-Chun Kung</b></u>, Chieh-Chih Wang, Wen-Chieh Lin<br>
      Under submission
      </p>

      <div class="paper" id="r2l3d">
        <a href="javascript:toggleblock('r2l3d_abs')">abstract</a> |
        <a href="https://youtu.be/d4kLRenhO_w">video</a>
        <p align="justify"> <i id="r2l3d_abs" style="display: none;">Radar presents a promising alternative to LiDAR in autonomous driving applications by showing several advantages, such as the ability to work under diverse weather conditions, direct doppler velocity estimation, and relatively low price.
        However, 4D radar outputs a 3D heatmap instead of a precise 3D shape of the surrounded environment. Moreover, the current state-of-the-art radar feature detection method, constant false alarm rate, only output sparse radar features.<br>

        &nbsp;&nbsp;In this paper, we show the potential to convert radar into the 3D shape measurement sensor by demonstrating the preliminary results of particular targets.
        We introduce the 3D-Unet model to the radar data and takes 3D shape measurements of LiDAR as the training ground truth.
        However, since the low-cost 4D radar only provides timestamp at second resolution and do not provides time interval between two consecutive data, it is particularly challenging to synchronize radar and LiDAR data. As a result, we only collect static data, including few classes of target objects and human postures, in our experiment.
        We demonstrate the preliminary experiment that estimates the 3D shape of particular targets from the radar module. The result shows the great potential of converting radar into the 3D shape measurement sensor if the data collection is more effective in the near future.</i></p>
        </pre>
      </div>
    </td>
  </tr> -->
  <!-- #################################### -->
  <tr>
    <td width="33%" valign="top" align="center">
      <a href="https://arxiv.org/abs/2103.07908"></a>
      <img src="images/ro_icra2021_small.gif" alt="sym" width="295" height="150" style="border-radius:3px; opacity:1.0; filter:alpha(opacity=80);">
    </td>
    <td width="67%" valign="top">
      <p><a href="https://arxiv.org/abs/2103.07908" id="RO">
      <heading>A Normal Distribution Transform-Based Radar Odometry Designed For Scanning and Automotive Radars</heading></a><br>
      <u><b>Pou-Chun Kung</b></u>, Chieh-Chih Wang, Wen-Chieh Lin<br>
      ICRA 2021, Xi'an
      </p>

      <div class="paper" id="ro">
        <a href="javascript:toggleblock('ro_abs')">abstract</a> |
        <a shape="rect" href="javascript:togglebib('ro')" class="togglebib">bibtex</a> |
        <a href="https://arxiv.org/abs/2103.07908">arXiv</a> |
        <a href="https://youtu.be/NhxXB0PjpJM">video</a> |
        <a href="https://youtu.be/xNrbPhDwXWE">presentation</a>

        <p align="justify"> <i id="ro_abs" style="display: none;">Existing radar sensors can be classified into automotive and scanning radars. While most radar odometry (RO) methods are only designed for a specific type of radar, our RO method adapts to both scanning and automotive radars. Our RO is simple yet effective, where the pipeline consists of thresholding, probabilistic submap building, and an NDT-based radar scan matching. The proposed RO has been tested on two public radar datasets: the Oxford Radar RobotCar dataset and the nuScenes dataset, which provide scanning and automotive radar data respectively. The results show that our approach surpasses state-of-the-art RO using either automotive or scanning radar by reducing translational error by 51% and 30%, respectively, and rotational error by 17% and 29%, respectively. Besides, we show that our RO achieves centimeter-level accuracy as lidar odometry, and automotive and scanning RO have similar accuracy.</i></p>
          <pre xml:space="preserve" style="display: none;">@inproceedings{kung2021normal,
              Author = {Pou-Chun Kung and
                        Chieh-Chih Wang and
                        Wen-Chieh Lin},
              Title = {A Normal Distribution Transform-Based Radar Odometry Designed For Scanning and Automotive Radars},
              Booktitle = {ICRA},
              Year = {2021}
            }
          </pre>
      </div>
    </td>
  </tr>

</tbody></table>

<!-- =================== Project =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Projects</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tbody>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <a href="#"></a>
    <img src="images/latitude.JPEG" align="center" alt="sym" width="200" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a id="LAT">
    <heading> LiDAR-Supervised Gaussian Splatting Data Synthesis</heading></a><br>
    Latitude AI <br>
    Worked under Nikita Jaipuria </br>
    </p>
    <!-- <div class="paper" id="radar_sdc">
      <a href="javascript:toggleblock('radar_sdc_abs')">abstract</a>
      <p align="justify"> <i id="radar_sdc_abs" style="display: none;"> Setup sensors on the vehicle and collect data in crowded urban.<br>
      Sensors:<br>
      - Lucid Camera x4<br>
      - Velodyne VLS-128 Lidar<br>
      - Ouster OS1-128 Lidar<br>
      - Baraja Lidar x2<br>
      - Navtech CIR504-X Radar<br>
      - Automotive Radar (Continental ARS 408-21) x2</i></p>
      </div> -->
  </td>
</tr>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <a href="#"></a>
    <img src="images/radar_car_crop.jpg" align="center" alt="sym" width="200" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a id="RADAR_SDC">
    <heading> Self-driving Car Sensor Setup and Data Collection</heading></a><br>
    Industrial Technology Research Institute (ITRI) Self-driving Car Team <br>
    Supervised by Chieh-Chih Wang </br>
    Collaborated with Sheng-Cheng Lee
    </p>
    <div class="paper" id="radar_sdc">
      <a href="javascript:toggleblock('radar_sdc_abs')">abstract</a>
      <p align="justify"> <i id="radar_sdc_abs" style="display: none;"> Setup sensors on the vehicle and collect data in crowded urban.<br>
      Sensors:<br>
      - Lucid Camera x4<br>
      - Velodyne VLS-128 Lidar<br>
      - Ouster OS1-128 Lidar<br>
      - Baraja Lidar x2<br>
      - Navtech CIR504-X Radar<br>
      - Automotive Radar (Continental ARS 408-21) x2</i></p>
      </div>
  </td>
</tr>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <img src="images/ucsd_racecar_top.gif" alt="sym" width="180" height="130" style="border-radius:3px">
    <img src="images/ucsd_racecar_bottom.gif" alt="sym" width="180" height="130" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a id="LO">
    <heading>Lidar Odometry with Pose Graph Optimization</heading></a><br>
    UCSD summer internship project <br>
    Advised by Prof. Nikolay Atanasov
    </p>
    <div class="paper" id="pnpdepth19">
      <a href="javascript:toggleblock('lo_abs')">abstract</a> |
      <a href="https://medium.com/@k3083518729/robot-localization-using-laser-scanner-and-pose-graph-optimization-fc40605bf5bc">webpage</a> |
      <a href="https://youtu.be/y1JVrlipT_4">video</a> |
      <a href="https://drive.google.com/file/d/1L9YGOCDzrqjGKEK2fVbfFkEpGchk4Gk3/view?usp=sharing">slide</a> |
      <a href="https://github.com/kungfrank/icp_gtsam_localization">code</a>
      <p align="justify"> <i id="lo_abs" style="display: none;"> In this project, we used Hokuyo Laser Scanner with scan matching (ICP) and factor graph optimization (GTSAM) to achieve Lidar odometry. We ran our algorithm onboard on Nvidia TX2 in Ubuntu18.04. Instead of doing consecutive Lidar scan matching only, we added the estimated transformation between the current and previous scans as constraints to optimize the robot pose.</i></p>
      </div>
  </td>
</tr>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <img src="images/ROScube-X_Expansion_L45.jpg" alt="sym" width="180" height="100" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a href="https://www.adlinktech.com/Products/ROS2_Solution/ROS2_Controller/ROScube-X#tab-ordering" id="ROS2">
    <heading>ROS2.0 ROScube-X product testing and user's manual writing</heading></a><br>
    Case job at <a href="https://www.adlinktech.com/en/index">ADLink</a> <br>
    </p>
    <div class="paper" id="ros2">
      <a href="https://www.adlinktech.com/Products/Download.ashx?type=MDownload&isManual=yes&file=1783%5cNeuron-SDK_50M-00017-1000_10.pdf">user's manual</a> |
      <a href="https://drive.google.com/file/d/15F227CgHAppjPxc3c9rhNww4rwBoJTwA/view">ros2 slide</a>
      </div>
  </td>
</tr>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <a href="#"></a>
    <img src="images/kbot_top_comp.gif" alt="sym" width="170" height="130" style="border-radius:3px">
    <img src="images/kbot_bottom_comp.gif" alt="sym" width="170" height="130" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a id="KNIGHTCAR">
    <heading>KnightCar: low-cost ROS autonomous platform</heading></a><br>
    Side project <br>
    Cooparated with Wei-Zhi Lin at INIKI ELECTRONICS CO. LTD. to commercialize KnightCar
    <!-- Cooparated with INIKI ELECTRONICS CO. LTD. to sale KnightCar as educational platform. -->
    </p>
    <div class="paper" id="knightcar">
      <a href="javascript:toggleblock('knightcar_abs')">abstract</a> |
      <a href="https://www.icshop.com.tw/product-page.php?26775">product webpage</a> |
      <a href="https://youtu.be/UlBHKnV47X4">video</a> |
      <a href="https://github.com/kungfrank/Knight_car">code</a>
      <p align="justify"> <i id="knightcar_abs" style="display: none;">Extended <a href="https://www.duckietown.org/">Duckiebot</a> with 2D Lidar to achieve 2D SLAM and navigation. Implemented Lidar particle filter SLAM, EKF SLAM, AMCL localization, and A* path planning on the robot. More than 200 KnightCars were saled as the teaching material in 5 colleges in a year.</i></p>
    </div>
  </td>
</tr>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <a href="#"></a>
    <img src="images/drone_indoor_nav_top_img.png" alt="sym" width="180" height="120" style="border-radius:3px">
    <img src="images/drone_indoor_nav_bottom_img.png" alt="sym" width="180" height="120" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a id="DRONESLAM">
    <heading>ARDrone Indoor 3D Mapping and Navigation with LSD-SLAM</heading></a><br>
    Undergraduate project <br>
    Advised by Prof. Kao-Shing Hwang
    </p>
    <div class="paper" id="droneslam">
      <a href="javascript:toggleblock('droneslam_abs')">abstract</a> |
      <a href="https://medium.com/@k3083518729/ardrone-indoor-slam-navigation-eec3812581dd">webpage</a> |
      <a href="https://youtu.be/PKo3p6c4t10">video</a> |
      <a href="https://github.com/Hypha-ROS/hypharos_ardrone_navigation">code</a>
      <p align="justify"> <i id="droneslam_abs" style="display: none;">Implemented LSD-SLAM with MultiSensor-Fusion EKF (fused with imu/sonar) on quadrotor to achieveautonomous indoor navigation. With this project, I won first place out of 26 teams in NSYSU College of Engineering Project Competition. </i></p>
    </div>
  </td>
</tr>

<!-- #################################### -->

<tr>
  <td width="33%" valign="top" align="center">
    <a href="#"></a>
    <img src="images/drone_tracking.gif" alt="sym" width="200" height="150" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a id="DRONETRACKING">
    <heading>ARDrone Face Recognition, Classification, and Tracking</heading></a><br>
    Undergraduate project <br>
    Advised by Prof. Kao-Shing Hwang
    </p>
    <div class="paper" id="DRONETRACKING">
      <a href="javascript:toggleblock('dronetracking_abs')">abstract</a>
      <p align="justify"> <i id="dronetracking_abs" style="display: none;">Implemented face recognition (Haar Cascades), classification (InceptionV3), tracking (KLT tracker) and PID controller on quadrotor to achieve people tracking. </i></p>
    </div>
  </td>
</tr>

<!--
<tr>
  <td width="33%" valign="top" align="center">
    <a href="http://aliensunmin.github.io/project/omni-cnn/"></a>
    <img src="images/led_drone.gif" alt="sym" width="200" height="120" style="border-radius:3px">
  </td>
  <td width="67%" valign="top">
    <p><a href="http://aliensunmin.github.io/project/omni-cnn/" id="OMNICNN18">
    <heading>Omnidirectional CNN for Visual Place Recognition and Navigation</heading></a><br>
    <u><b>Pou-Chun Kung</b></u>*, Hung-Jui Huang*, Juan-Ting Lin, Chan-Wei Hu, Kuo-Hao Zeng, Min Sun (* indicates equal contribution)<br>
    ICRA 2018, Brisbane
    </p>

    <div class="paper" id="omnicnn18">
      <a href="http://aliensunmin.github.io/project/omni-cnn/">webpage</a> |
      <a href="javascript:toggleblock('omnicnn18_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('omnicnn18')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/1803.04228">arXiv</a> |
      <a href="https://github.com/zswang666/Omni-CNN">code</a>
      <p align="justify"> <i id="omnicnn18_abs" style="display: none;">Visual place recognition is challenging, especially when only a few place exemplars are given. To mitigate the challenge, we consider place recognition method using omnidirectional cameras and propose a novel Omnidirectional Convolutional Neural Network (O-CNN) to handle severe camera pose variation. Given a visual input, the task of the O-CNN is not to retrieve the matched place exemplar, but to retrieve the closest place exemplar and estimate the relative distance between the input and the closest place. With the ability to estimate relative distance, a heuristic policy is proposed to navigate a robot to the retrieved closest place. Note that the network is designed to take advantage of the omnidirectional view by incorporating circular padding and rotation invariance. To train a powerful O-CNN, we build a virtual world for training on a large scale. We also propose a continuous lifted structured feature embedding loss to learn the concept of distance efficiently. Finally, our experimental results confirm that our method achieves state-of-the-art accuracy and speed with both the virtual world and real-world datasets.</i></p>
<pre xml:space="preserve" style="display: none;">@inproceedings{wang2019omnicnn,
Author = {Wang, Tsun-Hsuan and
          Huang, Hung-Jui and
          Lin, Juan-Ting and
          Hu, Chan-Wei and
          Zeng, Kuo-Hao and
          Sun, Min},
Title = {Omnidirectional CNN for Visual Place
         Recognition and Navigation},
Booktitle = {ICRA},
Year = {2018}
}
</pre>
    </div>
  </td>
</tr>
-->

</tbody></table>


<!-- =================== Other Projects =================== -->
<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Other Projects</sectionheading></td></tr>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tbody>
   <p align="center"> <i istyle="display: none; width: 30%;">Radar1. &nbsp;&nbsp;&nbsp;&nbsp; Radar2. &nbsp;&nbsp;&nbsp;&nbsp; Radar3.</i></p>
  <a href="javascript:toggleblock('pic_describe')"><img src="images/r2l.gif" title="radar" style="float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;"></a>
  <a href="javascript:toggleblock('pic_describe2')"><img src="images/r2l.gif" style="float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;"></a>
  <a href="javascript:toggleblock('pic_describe3')"><img src="images/r2l.gif" style="float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;"></a>
  <br><br><br><br>
  <p align="justify"> <i id="pic_describe" style="display: none; width: 30%;"> <br>Radar shows great potential for autonomous driving by accomplishing long-range sensing under diverse weather conditions.</i></p>
  <p align="justify"> <i id="pic_describe2" style="display: none; margin-left: 30%; width: 30%;">Radar shows great potential for autonomous driving by accomplishing long-range sensing under diverse weather conditions.</i></p>
  <p align="justify"> <i id="pic_describe3" style="display: none; margin-left: 60%; width: 30%;">Radar shows great potential for autonomous driving by accomplishing long-range sensing under diverse weather conditions.</i></p>
</tbody></table>
-->

<!-- =================== Teaching =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Teaching</sectionheading></td></tr>
</tbody></table>
<span style="line-height:0.7">
      <p style="text-align:left; padding-left:11px">
        Spring 2021,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TA, Human Centric Computing, NYCU
      </p>
      <p style="text-align:left; padding-left:11px">
        Fall 2020,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TA, Self-Driving Cars, NYCU
      </p>
      <p style="text-align:left; padding-left:11px">
        Spring 2020,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TA, Human Centric Computing, NYCU
      </p>
    </span><table width="100%" align="top" border="0" cellpadding="0">
  <tbody><tr>
    
  </tr>
</tbody></table>

<!-- =================== HONORS & AWARDS =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
  <tbody><tr><td><sectionheading>&nbsp;&nbsp;Honors & Awards</sectionheading></td></tr>
</tbody></table>
<span style="line-height:0.7">
      <p style="text-align:left; padding-left:11px">
        Spring 2021,&nbsp;&nbsp;<b>Recipient of The Phi Tau Phi Scholastic Honor</b> - Only award to 130 students every year (<0.4%)
      </p>
      <p style="text-align:left; padding-left:11px">
        Fall 2020,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>NYCU Academic Achievement Award</b> - Ranked 1st in a semester
      </p>
      <p style="text-align:left; padding-left:11px">
        Spring 2020,&nbsp;&nbsp;<b>NYCU Academic Achievement Award</b> - Ranked 1st in a semester
      </p>
      <p style="text-align:left; padding-left:11px">
        Fall 2019,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>NYCU Academic Achievement Award</b> - Ranked 1st in a semester
      </p>
      <p style="text-align:left; padding-left:11px">
        Fall 2018,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>NSYSU College of Engineering Project Champion</b> - First place out of 26 teams
      </p>
      <p style="text-align:left; padding-left:11px">
        Fall 2018,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>NSYSU Excellent Student Award</b> - Top 3 students in a semester
      </p>
      <p style="text-align:left; padding-left:11px">
        Spring 2017,&nbsp;&nbsp;<b>NSYSU Excellent Student Award</b> - Top 3 students in a semester
      </p>
    </span><table width="100%" align="top" border="0" cellpadding="0">
  <tbody><tr>

  </tr>
</tbody></table>

<!-- =================== Footnote =================== -->
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td><br><p align="right"><font size="2">
    A huge thanks to template from <a href="https://zswang666.github.io/">this</a>.
  </font></p></td></tr>
</tbody></table>

</td></tr>


<!-- =================== Javascript =================== -->
<script xml:space="preserve" language="JavaScript">
  hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('omnicnn18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pouring18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pnpdepth19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ccvnorm19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('p2pvg19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('v2vnet20_abs');
</script>




</tbody></table></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
